{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4a96b3-3936-4b69-856c-cd97396ae627",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d6f39c-53c8-422c-b7aa-160a1d6e94c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning_NN\\tfvenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "D:\\Learning_NN\\tfvenv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Press Ctrl+C to stop...\n",
      "RMS Energy: 0.026231674477458\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Left\n",
      "RMS Energy: 0.0038298615254461765\n",
      "Speech not Detected\n",
      "RMS Energy: 0.003756255144253373\n",
      "Speech not Detected\n",
      "RMS Energy: 0.004072780255228281\n",
      "Speech not Detected\n",
      "RMS Energy: 0.14202678203582764\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "Right\n",
      "RMS Energy: 0.002732691587880254\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0028137327171862125\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0027411894407123327\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0026794273871928453\n",
      "Speech not Detected\n",
      "RMS Energy: 0.10489621758460999\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Right\n",
      "RMS Energy: 0.0033203447237610817\n",
      "Speech not Detected\n",
      "RMS Energy: 0.003186509944498539\n",
      "Speech not Detected\n",
      "RMS Energy: 0.00341261038556695\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0038403894286602736\n",
      "Speech not Detected\n",
      "RMS Energy: 0.12576550245285034\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Down\n",
      "RMS Energy: 0.002686111954972148\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002485416829586029\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002408290281891823\n",
      "Speech not Detected\n",
      "RMS Energy: 0.09989899396896362\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Down\n",
      "RMS Energy: 0.0023767189122736454\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0026656538248062134\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002702457830309868\n",
      "Speech not Detected\n",
      "RMS Energy: 0.11241026222705841\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Start\n",
      "RMS Energy: 0.007749884855002165\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002634946024045348\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002718605101108551\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002534995088353753\n",
      "Speech not Detected\n",
      "RMS Energy: 0.09921180456876755\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Start\n",
      "RMS Energy: 0.002219249727204442\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0026097362861037254\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002559209242463112\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0025741916615515947\n",
      "Speech not Detected\n",
      "RMS Energy: 0.0029349562246352434\n",
      "Speech not Detected\n",
      "RMS Energy: 0.060955651104450226\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Start\n",
      "RMS Energy: 0.013458209112286568\n",
      "(1, 15, 35, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Right\n",
      "RMS Energy: 0.0033989055082201958\n",
      "Speech not Detected\n",
      "RMS Energy: 0.002919181250035763\n",
      "Speech not Detected\n",
      "\n",
      "Stopping recording...\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "import librosa\n",
    "import pyautogui\n",
    "\n",
    "# Parameters\n",
    "SAMPLE_RATE = 22050\n",
    "CHUNK_DURATION = 0.8\n",
    "CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD = 0.01\n",
    "\n",
    "# Global flag to control recording\n",
    "recording = True\n",
    "\n",
    "\n",
    "def Model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(15,35,1)))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.load_weights('audio_processing2.weights.h5')\n",
    "    return model\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "def processing(audio):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=22050, n_fft=1048, hop_length=512, n_mels=15)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    inputt = np.array(log_mel_spectrogram).reshape(1,15,35,1)\n",
    "    print(inputt.shape)\n",
    "    pred = model.predict(inputt)\n",
    "    outputt = np.argmax(pred,axis=1)\n",
    "    if outputt == 0:\n",
    "        pyautogui.press('up')\n",
    "        print('Jump')\n",
    "    elif outputt == 1:\n",
    "        pyautogui.press('down')\n",
    "        print('Down')\n",
    "    elif outputt == 2:\n",
    "        pyautogui.press('left')\n",
    "        print('Left')\n",
    "    elif outputt == 3:\n",
    "        pyautogui.press('right')\n",
    "        print('Right')\n",
    "    elif outputt == 4:\n",
    "        print('Start')\n",
    "        pyautogui.press('enter')\n",
    "    \n",
    "\n",
    "def is_speech(audio_data):\n",
    "    # Compute the root mean square (RMS) energy of the audio data\n",
    "    rms = np.sqrt(np.mean(np.square(audio_data)))\n",
    "    print(f\"RMS Energy: {rms}\")\n",
    "\n",
    "    # Determine if speech is present, based on RMS energy\n",
    "    return rms > ENERGY_THRESHOLD\n",
    "\n",
    "    \n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status, file=sys.stderr)\n",
    "    # Process audio data here\n",
    "    audio_data = indata[:, 0]\n",
    "    if is_speech(audio_data):\n",
    "        processing(audio_data)\n",
    "    else:\n",
    "        print('Speech not Detected')\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global recording\n",
    "    print(\"\\nStopping recording...\")\n",
    "    recording = False\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "\n",
    "global recording\n",
    "\n",
    "with sd.InputStream(samplerate=SAMPLE_RATE, channels=2, callback=callback, blocksize=CHUNK_SIZE) as stream:\n",
    "    print(\"Recording started. Press Ctrl+C to stop...\")\n",
    "    while recording:\n",
    "        time.sleep(CHUNK_DURATION)  # Sleep for the duration of each chunk to keep recording\n",
    "    print(\"Recording stopped.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
